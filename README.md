# ğŸš€ RESI: Rendered Embodiment of Social Interaction

ğŸ­ **An immersive neurofeedback-driven interactive installation** where real-time biometric data by two participants at a time fuels **generative audiovisual experiences**. Using EEG, PPG, GSR, and temperature sensors, RESI transforms human physiological states into **soundscapes, visuals, and interactive elements**, bridging neuroscience, art, and technology.

---

## âœ¨ Project Overview

ğŸ”¹ **Real-Time Neurofeedback**: Participants' brain activity, heart rate variability, and skin conductivity directly influence the audiovisual environment.  
ğŸ”¹ **Multi-Sensory Immersion**: Live projections, spatial audio, and adaptive lighting create a responsive, interactive space.  
ğŸ”¹ **Collaborative & Mindful**: Participants influence each otherâ€™s experiences through **inter-brain synchrony** and biometric data correlation.  
ğŸ”¹ **Dynamic Visualizations**: Immersive graphics that evolve with the synchrony between participants' signals, i.e. **emotional states, attention levels, and physiological responses**.  
ğŸ”¹ **Multi-Sensory Immersion**: Spatial audio, projection mapping, and adaptive lighting respond to the participants' brain activity and the quality of their social interaction.  
ğŸ”¹ **Dynamic Visualizations**: Immersive graphics that evolve with the synchrony between participants' signals, i.e. **emotional states, attention levels, and physiological responses**.

RESI is designed to encourage participants to engage in a meaningful and authentic interaction with each other, as the quality of their interaction directly influences the audio-visual experience they have together. With RESI, we create a space for participants to reflect on the nature of human interaction and the role of social connections in shaping our experiences and perceptions of the world. By encouraging self-awareness and connection, the installation aims to foster a deeper understanding of our shared humanity.

### ğŸ¨ **Inspired by**

ğŸ”¹ **Neural Aesthetics**: Explore the beauty of brainwaves and biometric signals
ğŸ”¹ **Human Connection**: Articulating shared experiences through physiological interaction
ğŸ”¹ **Interactive Art**: Engaging audiences in a dialogue with technology and self

---

## ğŸ§  How It Works

1ï¸âƒ£ **Sensors Capture Data**

- EEG, PPG, GSR, and temperature readings are acquired via OpenBCI and additional biosensors.  
  2ï¸âƒ£ **Processing & Analysis**
- Data is streamed in real-time using LabstreamingLayer (LSL), processed for **bandpower, coherence, HRV, and emotional states**.  
  3ï¸âƒ£ **Mapping to OSC/MIDI**
- Biometric data is transformed into control signals for generative audiovisual systems in **Max/MSP, Unity, or custom shaders**.  
  4ï¸âƒ£ **Real-Time Audiovisual Synthesis**
- Brainwaves shape particles, heart rate drives bass pulses, and skin conductivity sparks light bursts.

---

## ğŸ”§ Hardware & Software Stack

### ğŸ’¾ **Hardware**

- OpenBCI Cyton (8-16 channel EEG)
- PPG sensors
- GSR sensors
- Temperature sensors
- Spatialized speaker system & projection mapping

### ğŸ–¥ **Software**

- **Arduino IoT Cloud**: Sensor data aggregation
- **OpenSoundControl (OSC)**: Control signal transmission
- **LabstreamingLayer (LSL)**: Real-time biosignal streaming
- **Max/MSP / Ableton Live**: Sound synthesis & modulation
- **Unity / TouchDesigner**: Generative visuals & interaction
- **OpenBCI GUI**: EEG signal processing & visualization

---

## ğŸ›ï¸ System Architecture

### ğŸ“¡ **Data Flow**

```mermaid
graph TD;
    A[Sensors] -- Raw Signals --> B[DataProcessing];
    B[DataProcessing] -- Filtered Metrics --> C[OSC_Conversion];
    C -- Control Signals --> D[Visuals];
    C -- Control Signals --> E[Audio];
    D -- Generative Graphics --> Projector;
    E -- Ambient Soundscapes --> Speakers;
```

### ğŸ§¬ **Biometric Data Processing**

```mermaid
graph TD;
    A[EEG] -->|Bandpower| B[Alpha, Beta Power];
    A -->|Coherence| C[Inter-Brain Sync];
    D[PPG] -->|HRV| E[Heart Rate Variability];
    A -->|Spikes| F[Emotional Arousal];
    G[Temp] -->|Skin Temp| H[Emotional Valence];
    I[GSR] -->|Conductivity| J[Stress Levels];
    I -->|Stress Levels| L[Emotional State];
    I -->|Stress Levels| N[Emotional State Adjustment];
```

### ğŸ¨ **Audiovisual Effects Mapping**

```mermaid
graph TD;
    A[EEG_Alpha_Power] -->|Particle Density & Color Shifts| Effects;
    B[EEG_Beta_Power] -->|Particle Speed & Movement| Effects;
    C[HRV] -->|Ambient Drones & Scene Transitions| Effects;
    D[GSR_Spikes] -->|Short Bursts of Light & Sound Effects| Effects;
```

---

## ğŸ¨ Audiovisual Mapping

| Biometric Data                  | Visual Effects               | Audio Effects           |
| ------------------------------- | ---------------------------- | ----------------------- |
| EEG Alpha Power (8â€“12Hz)        | Wave patterns, color shifts  | Reverb, drone harmonics |
| EEG Beta Power (12â€“30Hz)        | Rapid particle movement      | Percussion modulations  |
| HRV (Heart Rate Variability)    | Scene transitions            | Ambient pulses          |
| GSR Spikes (Emotional Arousal)  | Light bursts, glitch effects | Distorted textures      |
| Temperature (Emotional Valence) | Color temperature shifts     | Harmonic shifts         |

---

## ğŸ—ï¸ Installation & Setup

ğŸš€ Setting up your own RESI experience? Follow the guide in Installation_Environment_Setup.md for:
âœ… Projector & speaker positioning
âœ… EEG & biometric sensor placement
âœ… Acoustic & lighting recommendations

---

## ğŸ› ï¸ Development Workflow

ğŸ‘¨â€ğŸ’» Want to contribute? Hereâ€™s the workflow:

Clone the repo:

```sh
git clone https://github.com/JoshPattani/ReSi_Rendered-Embodiment-of-Social-Interactions.git
cd ReSi_Rendered-Embodiment-of-Social-Interactions
```

Set up your Python environment:

```sh
pip install -r requirements.txt
```

Run the data pipeline:

```sh
python main.py
```

Launch OpenBCI GUI and start streaming data.
Open Max/MSP, Unity, or Ableton Live and connect to OSC signals.

---

## ğŸ¤¯ Future Enhancements

ğŸ”® Complexity of EEG synchrony mapping
ğŸ”® AI-driven emotional state prediction
ğŸ”® Expanded sensor suite (ECG, EDA, motion tracking)
ğŸ”® Haptic feedback integration

---

#ğŸ’¡ Credits & Inspiration
ğŸš€ Developed by: Cass Bliss, Josh Pattani, and Jazlin Rodriguez
ğŸ”— Related Projects: OpenBCI, Biofeedback VR, generative audiovisual synthesis

---

ğŸ™Œ Pull requests & collaborations welcome!
